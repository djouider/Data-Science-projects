{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":89366,"databundleVersionId":10333534,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:11:23.305456Z","iopub.execute_input":"2024-12-25T15:11:23.305669Z","iopub.status.idle":"2024-12-25T15:11:23.670849Z","shell.execute_reply.started":"2024-12-25T15:11:23.305649Z","shell.execute_reply":"2024-12-25T15:11:23.669864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor # Updated import\nfrom sklearn.metrics import accuracy_score,mean_squared_error,f1_score\n\n# Load datasets\ntrain_data = pd.read_csv('/kaggle/input/smart-urban-analytics/urban_development_dataset.csv')\ntest_data = pd.read_csv('/kaggle/input/smart-urban-analytics/urban_development_test_data.csv')\n\n# Identify common columns between training and test datasets\ncommon_columns = train_data.columns.intersection(test_data.columns)\n    \n# Select only the features present in both datasets\n# Ensure 'development_trend_score' is excluded if present\nif 'development_trend_score' in common_columns:\n    feature_columns = common_columns.drop('development_trend_score')\nelse:\n    feature_columns = common_columns\n\nX = train_data[feature_columns]\ny = train_data['development_trend_score']\n\ntest_features = test_data[feature_columns]\n\n# Handle missing values\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)\ntest_data_imputed = imputer.transform(test_features)\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_imputed)\ntest_data_scaled = scaler.transform(test_data_imputed)\n\n# Convert target variable to integer (if not already categorical)\ny_class = y.astype(int)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:11:23.672117Z","iopub.execute_input":"2024-12-25T15:11:23.672676Z","iopub.status.idle":"2024-12-25T15:11:24.389102Z","shell.execute_reply.started":"2024-12-25T15:11:23.672637Z","shell.execute_reply":"2024-12-25T15:11:24.388037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## using GridSearch to get the best parameters","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,y_class, random_state=100, test_size=0.6)\n        \n    #initializing the random forest\nrandomforest = RandomForestClassifier(random_state=60,max_depth=None,min_samples_leaf= 1, min_samples_split= 5, n_estimators= 100)\n\n\n    #using hyperparameter tunning \nparam_grid = {\n   'n_estimators': [100,150],  \n   'max_depth': [None, 3, 5],    \n   'min_samples_split': [2, 5, 10],  \n   'min_samples_leaf': [1, 2, 4],  \n   'bootstrap': [True, False] \n}\n\n    #initiallizing the modal\ngrid_search = GridSearchCV(estimator=randomforest, param_grid=param_grid, cv=5, scoring='f1_macro', verbose=2, n_jobs=-1)\n\n    # fitting the gridsearch\ngrid_search.fit(X_train, y_train)\n        \n    #printing the best result\nbest_params = grid_search.best_params_\nprint('for',X_scaled,'best parameters are',best_params)\n\n    #getting the score\nprint('best score is',grid_search.best_score_)\n        \n    #evaluatign the test\nprint('test score is',grid_search.score(X_test,y_test))\n\n    #evaluate the modal \nbest_rf = grid_search.best_estimator_\ny_pred = best_rf.predict(X_test)\n        \n    # training the modal with the best parameters\nrandomforest = RandomForestClassifier(**best_params,random_state=60,max_depth=None,min_samples_leaf= 1, min_samples_split= 5, n_estimators= 100)\n        \n    #training the modal\nrandomforest.fit(X_train,y_train)\n        \n    #prediciting\nprediction = randomforest.predict(X_test)\nprint('the predictions:',pd.DataFrame(prediction).value_counts())\n        \n    #getting the f1 value\nf1 = f1_score(y_test, prediction,average='macro')\n        \n    #accuraccy and mse \nscore = accuracy_score(y_test,prediction)\nmse = mean_squared_error(y_test,prediction)\n        \nprint('score =',score,'mse =',mse)\nprint('')\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(\"it took\",elapsed_time,\"seconds to execute\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:11:54.689848Z","iopub.execute_input":"2024-12-25T15:11:54.690212Z","iopub.status.idle":"2024-12-25T15:11:54.707538Z","shell.execute_reply.started":"2024-12-25T15:11:54.690175Z","shell.execute_reply":"2024-12-25T15:11:54.706392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## using xgbosst","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# Load data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\nX_test_mode = X_test.copy()\ncolumns = X_test.columns\n\n# filling the nulls using the mode\nfor c in columns:\n    X_test_mode.fillna(X_test_mode[c].mode()[0],inplace=True)\n\n# Convert to DMatrix (optimized data structure for XGBoost)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Define parameters\nparams = {\n    'booster': 'gbtree',\n    'max_depth': 3,                 \n    'learning_rate': 0.01,          \n    'n_estimators': 100,           \n    'subsample': 0.8,                \n    'colsample_bytree': 0.8,         \n    'gamma': 5  ,                 \n    'reg_alpha': 1,                \n    'reg_lambda': 3,               \n}\n\n# Train the model\nmodel = xgb.train(params, dtrain,num_boost_round=1 )\n\nX_train_rf = dtrain.get_data()\ny_train_rf = dtrain.get_label()\n\n# using the random forest \nrf = RandomForestRegressor(n_estimators=100,random_state = 60,min_samples_split=5)\nrf.fit(X_train_rf,y_train_rf)\n\n# Predict\ny_pred = rf.predict(X_test_mode)\ny_pred = np.round(y_pred)\n\nprint(\"MSE:\", mean_squared_error(y_test, y_pred))\nprint('score:',accuracy_score(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:11:24.390102Z","iopub.execute_input":"2024-12-25T15:11:24.39039Z","iopub.status.idle":"2024-12-25T15:11:54.559863Z","shell.execute_reply.started":"2024-12-25T15:11:24.390365Z","shell.execute_reply":"2024-12-25T15:11:54.558902Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## evaluation","metadata":{}},{"cell_type":"code","source":"# getting the testclass\n#test_data_scaled = xgb.DMatrix(test_data)\nfor c in columns:\n    test_data.fillna(test_data[c].mode()[0],inplace=True)\n\n# Predict on the test dataset\ntest_class_predictions = rf.predict(test_data)\n#test_class_predictions = model.predict(test_data_scaled)\n\ntest_class_predictions = np.round(test_class_predictions)\n\n# Prepare the submission file\nsubmission = pd.DataFrame({\n    \"ID\": test_data.index + 1,  # Assuming IDs are 1-based indices\n    \"development_trend_score\": test_class_predictions\n})\n\n# Save the submission file\nsubmission_file_path = '/kaggle/working/submission.csv'\nsubmission.to_csv(submission_file_path, index=False)\nprint(f\"Submission file saved at: {submission_file_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:14:52.489264Z","iopub.execute_input":"2024-12-25T15:14:52.489605Z","iopub.status.idle":"2024-12-25T15:14:52.600337Z","shell.execute_reply.started":"2024-12-25T15:14:52.489579Z","shell.execute_reply":"2024-12-25T15:14:52.599205Z"}},"outputs":[],"execution_count":null}]}